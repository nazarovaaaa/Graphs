{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "z-xI5qs4Y0lX"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сгенерируем графы для задачи классификации на графы-циклы и графы-пути (укажу постоянное число вершин, чтобы одинаково применить функцию поиска кратчайших путей)."
      ],
      "metadata": {
        "id": "VMs9v463ZLwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(size = 100, nodes = 30):\n",
        "    graph_list = [nx.cycle_graph(nodes) for i in range(10, size//2 + 10)]\n",
        "    graph_list.extend([nx.path_graph(nodes) for i in range(size//2 + 10, size + 10)])\n",
        "    y = [0 if i < (size//2 + 10) else 1 for i in range(size)]\n",
        "\n",
        "    return graph_list, y\n",
        "\n",
        "#разделим данные на train/test\n",
        "graph_list, y = create_dataset(300)\n",
        "train_graphs, test_graphs, y_train, y_test = train_test_split(graph_list, y, test_size=0.1, stratify=y)"
      ],
      "metadata": {
        "id": "X_7KJPauY6M0"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализую функцию ядра через кратчайшие расстояния в графе. Соберу вектор из кратчайших расстояний (от вершины 0 до каждой в графе, далее от 1 до каждой, кроме 0, и т.д.). В качестве ядровой функции оставляю скалярное произведение."
      ],
      "metadata": {
        "id": "cMtJpOv-CiPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shortest_path_kernel(train_graphs, test_graphs, nodes = 30):\n",
        "\n",
        "  #задаю начальные значения векторов\n",
        "  phi_train = np.zeros((len(train_graphs), nodes * (nodes - 1)))\n",
        "  phi_test = np.zeros((len(test_graphs), nodes * (nodes - 1)))\n",
        "\n",
        "  #рассчитываю для тренировочных графов\n",
        "  for i, graph in enumerate(train_graphs):\n",
        "    idx = 0\n",
        "    #заполняем вектор кратчайших расстояний между каждой парой вершин\n",
        "    for u in range(graph.number_of_nodes() - 1):\n",
        "      for v in range(u + 1, graph.number_of_nodes()):\n",
        "        phi_train[i][idx] = nx.shortest_path_length(graph, u, v)\n",
        "        idx += 1\n",
        "\n",
        "  #рассчитываю для тестовых графов\n",
        "  for i, graph in enumerate(test_graphs):\n",
        "    idx = 0\n",
        "    for u in range(graph.number_of_nodes() - 1):\n",
        "      for v in range(u + 1, graph.number_of_nodes()):\n",
        "        phi_test[i][idx] = nx.shortest_path_length(graph, u, v)\n",
        "        idx += 1\n",
        "\n",
        "  #вычисляю матрицы из ядровых функций\n",
        "  K_train = np.dot(phi_train, phi_train.T)\n",
        "  K_test = np.dot(phi_test, phi_train.T)\n",
        "  return K_train, K_test\n",
        "\n",
        "K_train_gk, K_test_gk = shortest_path_kernel(train_graphs, test_graphs)"
      ],
      "metadata": {
        "id": "_85GUrjEbNq8"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Организую перебор гиперпараметров SVC, не зависящих от типа ядра (некоторые применяются только к определенному типу ядра)."
      ],
      "metadata": {
        "id": "8IekGyvrCwh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#сделаю gridSearch по параметрам, которые могут меняться при использовании precomputed-ядра\n",
        "#размер кэша, параметр регуляризации и условие критерия остановки\n",
        "parameters = {'cache_size': [200, 300], 'C':[1, 5, 10], 'tol': [0.01, 0.001, 0.0001]}\n",
        "model1 = SVC(kernel ='precomputed', random_state =33)\n",
        "clf1 = GridSearchCV(model1, parameters)\n",
        "clf1.fit(K_train_gk, y_train)\n",
        "clf1.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvtV6B9tmBnv",
        "outputId": "1a513753-091c-4319-b57c-d857a301acf6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 1, 'cache_size': 200, 'tol': 0.01}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучу лучшую модель и оценю результаты с использованием accuracy, precision и recall."
      ],
      "metadata": {
        "id": "OR5ou3EeC2OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#выберу лучшую модель согласно перебору и посчитаю метрики\n",
        "best_model1 = SVC(C = 1, kernel ='precomputed', cache_size = 200, tol = 0.01, random_state =33)\n",
        "best_model1.fit(K_train_gk, y_train)\n",
        "y_pred1 = best_model1.predict(K_test_gk)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred1)}')\n",
        "print(f'Precision: {precision_score(y_test, y_pred1)}')\n",
        "print(f'Recall: {recall_score(y_test, y_pred1)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pr0tTadoRTX",
        "outputId": "e02e0408-ffcd-4e6e-c70d-bc75c8dd08f9"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Precision: 0.9333333333333333\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По результатам, показанным моделью, можно судить о том, что подобный способ определения ядра позволяет с достаточно высокой точностью определять структурные особенности графов (по крайней мере, на относительно небольшом множестве сгенерированных графов)."
      ],
      "metadata": {
        "id": "qSJKAQsAC74Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ядро Weisfeiler-Lehman**\n",
        "\n",
        "Здесь реализуется другая идея построения вектора phi. Предполагается итеративно раскрашивать (ставить метки) вершины графа, которые будут отражать их соседство. Таким образом, на k-ом шаге мы получим k-уровень соседства вершин. В целом, поскольку для N вершин максимальное число шагов между любыми двумя вершинами - N-1, то больше брать нет смысла. Однако, можно взять меньше, но тогда есть риск не рассмотреть часть структурных особенностей графа. Далее, вектор считается по числу вершин каждого цвета, полученных при построении графа."
      ],
      "metadata": {
        "id": "hNRreKKdChFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Weisfeiler_Lehman_kernel(train_graphs, test_graphs, iterations = 30):\n",
        "  phi_train = []\n",
        "  phi_test = []\n",
        "  #рассчитываю для тренировочных графов\n",
        "  for i, graph in enumerate(train_graphs):\n",
        "    phi_train_dict = {1: graph.number_of_nodes()} #задаю словарь для результирующего вектора (ключ - номер, значение - число вершин с таким номером)\n",
        "    hash = {'1': '1'} #задаю сводную хэш-таблицу (ключ - порядковый уникальный номер, значение - метка-индекс)\n",
        "    node_vals = dict() #задаем изначальный словарь со сквозной маркировкой узлов (ключ - вершина, значение - текущая метка)\n",
        "    for i in range(graph.number_of_nodes()):\n",
        "      node_vals[i] = 1\n",
        "    #print('node_values')\n",
        "    #for f in node_vals.keys():\n",
        "      #print(f'key {i} value {node_vals[i]}')\n",
        "    #в цикле по итерациям\n",
        "    for f in range(iterations):\n",
        "      #ограничиваю вектор определенным числом чисел (по 5 доп. ячеек на каждую итерацию)\n",
        "      for o in range(f*10):\n",
        "        phi_train_dict[o+2] = 0\n",
        "      temp_hash = dict() #хэш-таблица очередной итерации алгоритма (ключ - вершина, значение - ее индекс)\n",
        "      #в цикле для каждой вершины\n",
        "      for j in range(graph.number_of_nodes()):\n",
        "        temp_node_idx = str(node_vals[j]) + ',' #строка для конкатенации всех меток для вершины\n",
        "        node_neighbs = '' #строка для меток всех соседей\n",
        "        #выделяем соседей по матрице смежности\n",
        "        temp_node_adj_matrix = nx.adjacency_matrix(graph)[:, [j]].toarray().reshape(-1)\n",
        "        for k in range(len(temp_node_adj_matrix)):\n",
        "          #записываем метку каждого соседа\n",
        "          if temp_node_adj_matrix[k] == 1:\n",
        "            node_neighbs += str(node_vals[k])\n",
        "        #строим итоговый индекс для вершины (ее метка + строка из меток соседей)\n",
        "        temp_node_idx += ''.join(sorted(node_neighbs))\n",
        "        temp_hash[j] = temp_node_idx\n",
        "      #прохожусь по отсортированной хэш-таблице очередной итерации\n",
        "      for j in {k: v for k,v in sorted(temp_hash.items(), key=lambda item: item[1])}.keys():\n",
        "        #записываю очередное значение в сводную хэш-таблицу, если его там нет\n",
        "        if temp_hash[j] not in hash.values():\n",
        "          max_iter = len(hash) #определяю максимальное значение в сводной хэш-таблице\n",
        "          hash[max_iter + 1] = temp_hash[j]\n",
        "      #print(f'hash')\n",
        "      #for j in hash.keys():\n",
        "        #print(f'key {j} value {hash[j]}')\n",
        "      #далее снова проходусь по собранной хэш-таблице очередной итерации\n",
        "      for key, value in temp_hash.items():\n",
        "        hash_item = list(filter(lambda x: hash[x] == value, hash))[0] #получаю уникальный номер из сводной таблицы для текущей метки-индекса\n",
        "        #обновляю словарь с маркировкой узлов значением в сводной хэш-таблице\n",
        "        node_vals[key] = hash_item\n",
        "        #обновляю словарь для результирующего вектора\n",
        "        if hash_item in phi_train_dict:\n",
        "          phi_train_dict[int(hash_item)] += 1\n",
        "        #else:\n",
        "          #phi_train_dict[int(hash_item)] = 1\n",
        "      #print(f'node_vals')\n",
        "      #for j in node_vals.keys():\n",
        "        #print(f'key {j} value {node_vals[j]} type {type(node_vals[j])}')\n",
        "      #print(f'phi_train_dict')\n",
        "      #for j in phi_train_dict.keys():\n",
        "        #print(f'key {j} type {type(j)} value {phi_train_dict[j]} type {type(phi_train_dict[j])}')\n",
        "\n",
        "    sorted_dict = sorted(phi_train_dict.items())\n",
        "    temp_phi_train = np.zeros(len(phi_train_dict.items()))\n",
        "    for i in range(len(temp_phi_train)):\n",
        "      temp_phi_train[i] = sorted_dict[i][1]\n",
        "    #print(phi_train)\n",
        "    phi_train.append(temp_phi_train)\n",
        "\n",
        "  #рассчитываю то же самое для тестовых графов\n",
        "  for i, graph in enumerate(test_graphs):\n",
        "    phi_test_dict = {1: graph.number_of_nodes()} #задаю словарь для результирующего вектора (ключ - номер, значение - число вершин с таким номером)\n",
        "    hash = {'1': '1'} #задаю сводную хэш-таблицу (ключ - порядковый уникальный номер, значение - метка-индекс)\n",
        "    node_vals = dict() #задаем изначальный словарь со сквозной маркировкой узлов (ключ - вершина, значение - текущая метка)\n",
        "    for i in range(graph.number_of_nodes()):\n",
        "      node_vals[i] = 1\n",
        "    #print('node_values')\n",
        "    #for f in node_vals.keys():\n",
        "      #print(f'key {i} value {node_vals[i]}')\n",
        "    #в цикле по итерациям\n",
        "    for f in range(iterations):\n",
        "      #ограничиваю вектор определенным числом чисел (по 5 доп. ячеек на каждую итерацию)\n",
        "      for o in range(f*10):\n",
        "        phi_test_dict[o+2] = 0\n",
        "      temp_hash = dict() #хэш-таблица очередной итерации алгоритма (ключ - вершина, значение - ее индекс)\n",
        "      #в цикле для каждой вершины\n",
        "      for j in range(graph.number_of_nodes()):\n",
        "        temp_node_idx = str(node_vals[j]) + ',' #строка для конкатенации всех меток для вершины\n",
        "        node_neighbs = '' #строка для меток всех соседей\n",
        "        #выделяем соседей по матрице смежности\n",
        "        temp_node_adj_matrix = nx.adjacency_matrix(graph)[:, [j]].toarray().reshape(-1)\n",
        "        for k in range(len(temp_node_adj_matrix)):\n",
        "          #записываем метку каждого соседа\n",
        "          if temp_node_adj_matrix[k] == 1:\n",
        "            node_neighbs += str(node_vals[k])\n",
        "        #строим итоговый индекс для вершины (ее метка + строка из меток соседей)\n",
        "        temp_node_idx += ''.join(sorted(node_neighbs))\n",
        "        temp_hash[j] = temp_node_idx\n",
        "      #прохожусь по отсортированной хэш-таблице очередной итерации\n",
        "      for j in {k: v for k,v in sorted(temp_hash.items(), key=lambda item: item[1])}.keys():\n",
        "        #записываю очередное значение в сводную хэш-таблицу, если его там нет\n",
        "        if temp_hash[j] not in hash.values():\n",
        "          max_iter = len(hash) #определяю максимальное значение в сводной хэш-таблице\n",
        "          hash[max_iter + 1] = temp_hash[j]\n",
        "      #print(f'hash')\n",
        "      #for j in hash.keys():\n",
        "        #print(f'key {j} value {hash[j]}')\n",
        "      #далее снова проходусь по собранной хэш-таблице очередной итерации\n",
        "      for key, value in temp_hash.items():\n",
        "        hash_item = list(filter(lambda x: hash[x] == value, hash))[0] #получаю уникальный номер из сводной таблицы для текущей метки-индекса\n",
        "        #обновляю словарь с маркировкой узлов значением в сводной хэш-таблице\n",
        "        node_vals[key] = hash_item\n",
        "        #обновляю словарь для результирующего вектора\n",
        "        if hash_item in phi_test_dict:\n",
        "          phi_test_dict[int(hash_item)] += 1\n",
        "        #else:\n",
        "          #phi_test_dict[int(hash_item)] = 1\n",
        "      #print(f'node_vals')\n",
        "      #for j in node_vals.keys():\n",
        "        #print(f'key {j} value {node_vals[j]} type {type(node_vals[j])}')\n",
        "      #print(f'phi_train_dict')\n",
        "      #for j in phi_train_dict.keys():\n",
        "        #print(f'key {j} type {type(j)} value {phi_train_dict[j]} type {type(phi_train_dict[j])}')\n",
        "\n",
        "    sorted_dict = sorted(phi_test_dict.items())\n",
        "    temp_phi_test = np.zeros(len(phi_test_dict.items()))\n",
        "    for i in range(len(temp_phi_test)):\n",
        "      temp_phi_test[i] = sorted_dict[i][1]\n",
        "    #print(phi_train)\n",
        "    phi_test.append(temp_phi_test)\n",
        "\n",
        "  #вычисляю матрицы из ядровых функций\n",
        "  K_train = np.dot(np.array(phi_train[:][:]), np.array(phi_train[:][:]).T)\n",
        "  K_test = np.dot(np.array(phi_test[:][:]), np.array(phi_train[:][:]).T)\n",
        "  return K_train, K_test"
      ],
      "metadata": {
        "id": "4PXKi3OoD2B_"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Протестирую полученную функцию: возьму немного больше датасет и для условности - 10 итераций алгоритма раскраски."
      ],
      "metadata": {
        "id": "qur9i2_oAUPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_list, y = create_dataset(500)\n",
        "train_graphs, test_graphs, y_train, y_test = train_test_split(graph_list, y, test_size=0.1, stratify=y)\n",
        "K_train_gk2, K_test_gk2 = Weisfeiler_Lehman_kernel(train_graphs, test_graphs, 10)\n",
        "model2 = SVC(C = 1, kernel ='precomputed', cache_size = 200, tol = 0.01, random_state =33)\n",
        "model2.fit(K_train_gk2, y_train)\n",
        "y_pred2 = model2.predict(K_test_gk2)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred2)}')\n",
        "print(f'Precision: {precision_score(y_test, y_pred2)}')\n",
        "print(f'Recall: {recall_score(y_test, y_pred2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxEoZJ9O_f7a",
        "outputId": "d94ccb5a-3b5d-431c-9bc6-6ab2cb81b2b6"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.98\n",
            "Precision: 0.96\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По результатам можно сделать следующие выводы:\n",
        "*   функция подсчета ядра работает дольше вследствие рекурсивного прохода по каждой из вершин и построении хэш-таблицы на каждом шаге;\n",
        "*   выше точность предсказания по сравнению с ядром кратчайших путей;\n",
        "*   возможность находить больше тонких различий между графами (поскольку обрабатываются не только пути, но и соседство вершин вместе с их метками).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MoMuK1ESAa9V"
      }
    }
  ]
}