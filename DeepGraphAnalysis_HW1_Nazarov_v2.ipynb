{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z-xI5qs4Y0lX"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сгенерируем графы для задачи классификации на графы-циклы и графы-пути (укажу постоянное число вершин, чтобы одинаково применить функцию поиска кратчайших путей)."
      ],
      "metadata": {
        "id": "VMs9v463ZLwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(size = 100, nodes = 30):\n",
        "    graph_list = [nx.cycle_graph(nodes) for i in range(10, size//2 + 10)]\n",
        "    graph_list.extend([nx.path_graph(nodes) for i in range(size//2 + 10, size + 10)])\n",
        "    y = [0 if i < (size//2 + 10) else 1 for i in range(size)]\n",
        "\n",
        "    return graph_list, y\n",
        "\n",
        "#разделим данные на train/test\n",
        "graph_list, y = create_dataset(300)\n",
        "train_graphs, test_graphs, y_train, y_test = train_test_split(graph_list, y, test_size=0.1, stratify=y)"
      ],
      "metadata": {
        "id": "X_7KJPauY6M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Также попробую использовать другой генератор графов для классификации (между графами в форме звезд и лестниц)."
      ],
      "metadata": {
        "id": "HedQVtCyhDdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset2(size = 100, nodes = 30):\n",
        "    graph_list = [nx.star_graph(nodes) for i in range(10, size//2 + 10)]\n",
        "    graph_list.extend([nx.ladder_graph(nodes) for i in range(size//2 + 10, size + 10)])\n",
        "    y = [0 if i < (size//2 + 10) else 1 for i in range(size)]\n",
        "\n",
        "    return graph_list, y\n",
        "\n",
        "#разделим данные на train/test\n",
        "graph_list2, y2 = create_dataset2(300)\n",
        "train_graphs2, test_graphs2, y_train2, y_test2 = train_test_split(graph_list2, y2, test_size=0.1, stratify=y2)"
      ],
      "metadata": {
        "id": "aQORwsIghGkR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализую функцию ядра через кратчайшие расстояния в графе. Соберу вектор из кратчайших расстояний (от вершины 0 до каждой в графе, далее от 1 до каждой, кроме 0, и т.д.). В качестве ядровой функции оставляю скалярное произведение."
      ],
      "metadata": {
        "id": "cMtJpOv-CiPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shortest_path_kernel(train_graphs, test_graphs, nodes = 30):\n",
        "\n",
        "  #определяю максимальную длину векторов phi\n",
        "  max_len_phi = 0\n",
        "  for i, graph in enumerate(train_graphs):\n",
        "    for u in range(graph.number_of_nodes() - 1):\n",
        "      for v in range(u+1, graph.number_of_nodes()):\n",
        "        temp_max_len_phi = nx.shortest_path_length(graph, u ,v)\n",
        "        if temp_max_len_phi > max_len_phi:\n",
        "          max_len_phi = temp_max_len_phi\n",
        "  for i, graph in enumerate(test_graphs):\n",
        "    for u in range(graph.number_of_nodes() - 1):\n",
        "      for v in range(u+1, graph.number_of_nodes()):\n",
        "        temp_max_len_phi = nx.shortest_path_length(graph, u ,v)\n",
        "        if temp_max_len_phi > max_len_phi:\n",
        "          max_len_phi = temp_max_len_phi\n",
        "\n",
        "  #задаю начальные значения векторов\n",
        "  phi_train = np.zeros((len(train_graphs), max_len_phi))\n",
        "  phi_test = np.zeros((len(test_graphs), max_len_phi))\n",
        "\n",
        "  #рассчитываю для тренировочных графов\n",
        "  for i, graph in enumerate(train_graphs):\n",
        "    #заполняем вектор количества кратчайших расстояний разной длины\n",
        "    for u in range(graph.number_of_nodes() - 1):\n",
        "      for v in range(u + 1, graph.number_of_nodes()):\n",
        "        phi_train[i][nx.shortest_path_length(graph, u, v) - 1] += 1\n",
        "\n",
        "  #рассчитываю для тестовых графов\n",
        "  for i, graph in enumerate(test_graphs):\n",
        "    #заполняем вектор количества кратчайших расстояний разной длины\n",
        "    for u in range(graph.number_of_nodes() - 1):\n",
        "      for v in range(u + 1, graph.number_of_nodes()):\n",
        "        phi_test[i][nx.shortest_path_length(graph, u, v) - 1] += 1\n",
        "\n",
        "  #вычисляю матрицы из ядровых функций\n",
        "  K_train = np.dot(phi_train, phi_train.T)\n",
        "  K_test = np.dot(phi_test, phi_train.T)\n",
        "  return K_train, K_test\n",
        "\n",
        "K_train_gk, K_test_gk = shortest_path_kernel(train_graphs, test_graphs)\n",
        "K_train_gk2, K_test_gk2 = shortest_path_kernel(train_graphs2, test_graphs2)"
      ],
      "metadata": {
        "id": "_85GUrjEbNq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Организую перебор гиперпараметров SVC, не зависящих от типа ядра (некоторые применяются только к определенному типу ядра)."
      ],
      "metadata": {
        "id": "8IekGyvrCwh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#сделаю gridSearch по параметрам, которые могут меняться при использовании precomputed-ядра\n",
        "#размер кэша, параметр регуляризации и условие критерия остановки\n",
        "parameters = {'cache_size': [200, 300], 'C':[1, 5, 10], 'tol': [0.01, 0.001, 0.0001]}\n",
        "model1 = SVC(kernel ='precomputed', random_state =33)\n",
        "clf1 = GridSearchCV(model1, parameters)\n",
        "clf1.fit(K_train_gk, y_train)\n",
        "clf1.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvtV6B9tmBnv",
        "outputId": "3e542e77-3742-444b-ca08-3d6cb86e0041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 1, 'cache_size': 200, 'tol': 0.01}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучу лучшую модель и оценю результаты с использованием accuracy, precision и recall."
      ],
      "metadata": {
        "id": "OR5ou3EeC2OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#выберу лучшую модель согласно перебору и посчитаю метрики\n",
        "best_model1 = SVC(C = 1, kernel ='precomputed', cache_size = 200, tol = 0.01, random_state =33)\n",
        "best_model1.fit(K_train_gk, y_train)\n",
        "y_pred1 = best_model1.predict(K_test_gk)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred1)}')\n",
        "print(f'Precision: {precision_score(y_test, y_pred1)}')\n",
        "print(f'Recall: {recall_score(y_test, y_pred1)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pr0tTadoRTX",
        "outputId": "4d1257e6-d775-4f33-acad-ecb18153b54a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Precision: 0.9333333333333333\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#выберу лучшую модель согласно перебору и посчитаю метрики\n",
        "best_model12 = SVC(C = 1, kernel ='precomputed', cache_size = 200, tol = 0.01, random_state =33)\n",
        "best_model12.fit(K_train_gk2, y_train2)\n",
        "y_pred12 = best_model12.predict(K_test_gk2)\n",
        "print(f'Accuracy: {accuracy_score(y_test2, y_pred12)}')\n",
        "print(f'Precision: {precision_score(y_test2, y_pred12)}')\n",
        "print(f'Recall: {recall_score(y_test2, y_pred12)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXiR1Sg5hZHl",
        "outputId": "bf03cc74-c34d-4b2e-a066-467f2f7d5bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Precision: 0.9333333333333333\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По результатам, показанным моделью, можно судить о том, что подобный способ определения ядра позволяет с достаточно высокой точностью определять структурные особенности графов (по крайней мере, на относительно небольшом множестве сгенерированных графов).\n",
        "\n",
        "После того, как были взяты другие типы графов и сгенерированы другие множества для классификации - алгоритм справился не хуже."
      ],
      "metadata": {
        "id": "qSJKAQsAC74Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ядро Weisfeiler-Lehman**\n",
        "\n",
        "Здесь реализуется другая идея построения вектора phi. Предполагается итеративно раскрашивать (ставить метки) вершины графа, которые будут отражать их соседство. Таким образом, на k-ом шаге мы получим k-уровень соседства вершин. В целом, поскольку для N вершин максимальное число шагов между любыми двумя вершинами - N-1, то больше брать нет смысла. Однако, можно взять меньше, но тогда есть риск не рассмотреть часть структурных особенностей графа. Далее, вектор считается по числу вершин каждого цвета, полученных при построении графа."
      ],
      "metadata": {
        "id": "hNRreKKdChFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Weisfeiler_Lehman_kernel(train_graphs, test_graphs, iterations = 30):\n",
        "  phi_train = []\n",
        "  phi_test = []\n",
        "  #рассчитываю для тренировочных графов\n",
        "  for i, graph in enumerate(train_graphs):\n",
        "    phi_train_dict = {1: graph.number_of_nodes()} #задаю словарь для результирующего вектора (ключ - номер, значение - число вершин с таким номером)\n",
        "    hash = {'1': '1'} #задаю сводную хэш-таблицу (ключ - порядковый уникальный номер, значение - метка-индекс)\n",
        "    node_vals = dict() #задаем изначальный словарь со сквозной маркировкой узлов (ключ - вершина, значение - текущая метка)\n",
        "    for i in range(graph.number_of_nodes()):\n",
        "      node_vals[i] = 1\n",
        "    #в цикле по итерациям\n",
        "    for f in range(iterations):\n",
        "      #ограничиваю вектор определенным числом чисел (по 5 доп. ячеек на каждую итерацию)\n",
        "      for o in range(f*10):\n",
        "        phi_train_dict[o+2] = 0\n",
        "      temp_hash = dict() #хэш-таблица очередной итерации алгоритма (ключ - вершина, значение - ее индекс)\n",
        "      #в цикле для каждой вершины\n",
        "      for j in range(graph.number_of_nodes()):\n",
        "        temp_node_idx = str(node_vals[j]) + ',' #строка для конкатенации всех меток для вершины\n",
        "        node_neighbs = '' #строка для меток всех соседей\n",
        "        #выделяем соседей по матрице смежности (использую функцию neighbors вместо слепого перебора по матрице)\n",
        "        for k in graph.neighbors(j):\n",
        "          node_neighbs += str(node_vals[k])\n",
        "        #строим итоговый индекс для вершины (ее метка + строка из меток соседей)\n",
        "        temp_node_idx += ''.join(sorted(node_neighbs))\n",
        "        temp_hash[j] = temp_node_idx\n",
        "      #прохожусь по отсортированной хэш-таблице очередной итерации\n",
        "      for j in {k: v for k,v in sorted(temp_hash.items(), key=lambda item: item[1])}.keys():\n",
        "        #записываю очередное значение в сводную хэш-таблицу, если его там нет\n",
        "        if temp_hash[j] not in hash.values():\n",
        "          max_iter = len(hash) #определяю максимальное значение в сводной хэш-таблице\n",
        "          hash[max_iter + 1] = temp_hash[j]\n",
        "      #далее снова проходусь по собранной хэш-таблице очередной итерации\n",
        "      for key, value in temp_hash.items():\n",
        "        hash_item = list(filter(lambda x: hash[x] == value, hash))[0] #получаю уникальный номер из сводной таблицы для текущей метки-индекса\n",
        "        #обновляю словарь с маркировкой узлов значением в сводной хэш-таблице\n",
        "        node_vals[key] = hash_item\n",
        "        #обновляю словарь для результирующего вектора\n",
        "        if hash_item in phi_train_dict:\n",
        "          phi_train_dict[int(hash_item)] += 1\n",
        "\n",
        "    sorted_dict = sorted(phi_train_dict.items())\n",
        "    temp_phi_train = np.zeros(len(phi_train_dict.items()))\n",
        "    for i in range(len(temp_phi_train)):\n",
        "      temp_phi_train[i] = sorted_dict[i][1]\n",
        "    #print(phi_train)\n",
        "    phi_train.append(temp_phi_train)\n",
        "\n",
        "  #рассчитываю то же самое для тестовых графов\n",
        "  for i, graph in enumerate(test_graphs):\n",
        "    phi_test_dict = {1: graph.number_of_nodes()} #задаю словарь для результирующего вектора (ключ - номер, значение - число вершин с таким номером)\n",
        "    hash = {'1': '1'} #задаю сводную хэш-таблицу (ключ - порядковый уникальный номер, значение - метка-индекс)\n",
        "    node_vals = dict() #задаем изначальный словарь со сквозной маркировкой узлов (ключ - вершина, значение - текущая метка)\n",
        "    for i in range(graph.number_of_nodes()):\n",
        "      node_vals[i] = 1\n",
        "    #в цикле по итерациям\n",
        "    for f in range(iterations):\n",
        "      #ограничиваю вектор определенным числом чисел (по 5 доп. ячеек на каждую итерацию)\n",
        "      for o in range(f*10):\n",
        "        phi_test_dict[o+2] = 0\n",
        "      temp_hash = dict() #хэш-таблица очередной итерации алгоритма (ключ - вершина, значение - ее индекс)\n",
        "      #в цикле для каждой вершины\n",
        "      for j in range(graph.number_of_nodes()):\n",
        "        temp_node_idx = str(node_vals[j]) + ',' #строка для конкатенации всех меток для вершины\n",
        "        node_neighbs = '' #строка для меток всех соседей\n",
        "        #выделяем соседей по матрице смежности (использую функцию neighbors вместо слепого перебора по матрице)\n",
        "        for k in graph.neighbors(j):\n",
        "          node_neighbs += str(node_vals[k])\n",
        "        #строим итоговый индекс для вершины (ее метка + строка из меток соседей)\n",
        "        temp_node_idx += ''.join(sorted(node_neighbs))\n",
        "        temp_hash[j] = temp_node_idx\n",
        "      #прохожусь по отсортированной хэш-таблице очередной итерации\n",
        "      for j in {k: v for k,v in sorted(temp_hash.items(), key=lambda item: item[1])}.keys():\n",
        "        #записываю очередное значение в сводную хэш-таблицу, если его там нет\n",
        "        if temp_hash[j] not in hash.values():\n",
        "          max_iter = len(hash) #определяю максимальное значение в сводной хэш-таблице\n",
        "          hash[max_iter + 1] = temp_hash[j]\n",
        "      for key, value in temp_hash.items():\n",
        "        hash_item = list(filter(lambda x: hash[x] == value, hash))[0] #получаю уникальный номер из сводной таблицы для текущей метки-индекса\n",
        "        #обновляю словарь с маркировкой узлов значением в сводной хэш-таблице\n",
        "        node_vals[key] = hash_item\n",
        "        #обновляю словарь для результирующего вектора\n",
        "        if hash_item in phi_test_dict:\n",
        "          phi_test_dict[int(hash_item)] += 1\n",
        "\n",
        "    sorted_dict = sorted(phi_test_dict.items())\n",
        "    temp_phi_test = np.zeros(len(phi_test_dict.items()))\n",
        "    for i in range(len(temp_phi_test)):\n",
        "      temp_phi_test[i] = sorted_dict[i][1]\n",
        "    #print(phi_train)\n",
        "    phi_test.append(temp_phi_test)\n",
        "\n",
        "  #вычисляю матрицы из ядровых функций\n",
        "  K_train = np.dot(np.array(phi_train[:][:]), np.array(phi_train[:][:]).T)\n",
        "  K_test = np.dot(np.array(phi_test[:][:]), np.array(phi_train[:][:]).T)\n",
        "  return K_train, K_test"
      ],
      "metadata": {
        "id": "4PXKi3OoD2B_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Протестирую полученную функцию: возьму немного больше датасет и для условности - 10 итераций алгоритма раскраски."
      ],
      "metadata": {
        "id": "qur9i2_oAUPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_list, y = create_dataset(500)\n",
        "train_graphs, test_graphs, y_train, y_test = train_test_split(graph_list, y, test_size=0.1, stratify=y)\n",
        "K_train_gk2, K_test_gk2 = Weisfeiler_Lehman_kernel(train_graphs, test_graphs, 10)\n",
        "model2 = SVC(C = 1, kernel ='precomputed', cache_size = 200, tol = 0.01, random_state =33)\n",
        "model2.fit(K_train_gk2, y_train)\n",
        "y_pred2 = model2.predict(K_test_gk2)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred2)}')\n",
        "print(f'Precision: {precision_score(y_test, y_pred2)}')\n",
        "print(f'Recall: {recall_score(y_test, y_pred2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxEoZJ9O_f7a",
        "outputId": "960c8286-1817-459b-a793-1e9860fa0c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.98\n",
            "Precision: 0.96\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробую классифицировать графы-звезды и графы-лестницы."
      ],
      "metadata": {
        "id": "_xeLhEztmFoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_list, y = create_dataset2(500)\n",
        "train_graphs, test_graphs, y_train, y_test = train_test_split(graph_list, y, test_size=0.1, stratify=y)\n",
        "K_train_gk2, K_test_gk2 = Weisfeiler_Lehman_kernel(train_graphs, test_graphs, 10)\n",
        "model2 = SVC(C = 1, kernel ='precomputed', cache_size = 200, tol = 0.01, random_state =33)\n",
        "model2.fit(K_train_gk2, y_train)\n",
        "y_pred2 = model2.predict(K_test_gk2)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred2)}')\n",
        "print(f'Precision: {precision_score(y_test, y_pred2)}')\n",
        "print(f'Recall: {recall_score(y_test, y_pred2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyE_P3v_l9Xz",
        "outputId": "7272e61d-4241-4934-f4f8-51a1378a92c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.96\n",
            "Precision: 0.9230769230769231\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По результатам можно сделать следующие выводы:\n",
        "*   функция подсчета ядра работает дольше вследствие рекурсивного прохода по каждой из вершин и построении хэш-таблицы на каждом шаге;\n",
        "*   выше точность предсказания по сравнению с ядром кратчайших путей;\n",
        "*   возможность находить больше тонких различий между графами (поскольку обрабатываются не только пути, но и соседство вершин вместе с их метками).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MoMuK1ESAa9V"
      }
    }
  ]
}